{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.svm import LinearSVC\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pavan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data=pd.read_csv('IMDB-Dataset.csv')\n",
    "dat=data.copy()\n",
    "final=dat.dropna()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=final.drop(['sentiment'],axis=1)\n",
    "y=final[\"sentiment\"]\n",
    "for i in y.index:\n",
    "  if y[i]==\"positive\":\n",
    "    y[i]=1\n",
    "  elif y[i]==\"negative\":\n",
    "    y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = dat['review'].loc[1]\n",
    "soup = BeautifulSoup(review, \"html.parser\")\n",
    "review = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test, train_data_label, test_data_label = train_test_split(dat['review'], dat['sentiment'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label = (train_data_label.replace({'positive': 1, 'negative': 0})).values\n",
    "test_data_label  = (test_data_label.replace({'positive': 1, 'negative': 0})).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = []\n",
    "corpus_test  = []\n",
    "with open('finalReview.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"review\", \"sentiment\"])\n",
    "    for i in range(dataset_train.shape[0]):\n",
    "        soup = BeautifulSoup(dataset_train.iloc[i], \"html.parser\")\n",
    "        review = soup.get_text()\n",
    "        review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "        lem = WordNetLemmatizer()\n",
    "        review = [lem.lemmatize(word) for word in review]\n",
    "        review = ' '.join(review)\n",
    "        writer.writerow([review, train_data_label[i]])\n",
    "        corpus_train.append(review)\n",
    "      \n",
    "    for j in range(dataset_test.shape[0]):\n",
    "        soup = BeautifulSoup(dataset_test.iloc[j], \"html.parser\")\n",
    "        review = soup.get_text()\n",
    "        review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "        lem = WordNetLemmatizer()\n",
    "        review = [lem.lemmatize(word) for word in review]\n",
    "        review = ' '.join(review)\n",
    "        writer.writerow([review, test_data_label[j]])\n",
    "        corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\n",
    "tfidf_vec_test = tfidf_vec.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "linear_svc = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc.fit(tfidf_vec_train, train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = linear_svc.predict(tfidf_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "check =pd.read_csv('moviereview.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Data = check['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Checker_Data = tfidf_vec.transform(Test_Data)\n",
    "predict_test = linear_svc.predict(Checker_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['final_model.sav']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "filename = 'final_model.sav'\n",
    "joblib.dump(linear_svc,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = pd.DataFrame(predict_test,  columns=['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = pd.concat([check, predict_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe.to_csv('Result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Unnamed: 0 Movie_name     Genre  \\\n",
       "0             0  Peninsula  Thriller   \n",
       "1             1  Peninsula  Thriller   \n",
       "2             2  Peninsula  Thriller   \n",
       "3             3  Peninsula  Thriller   \n",
       "4             4  Come Play   Mystery   \n",
       "..          ...        ...       ...   \n",
       "410         410      Bhoot    Horror   \n",
       "411         411      Bhoot    Horror   \n",
       "412         412      Bhoot    Horror   \n",
       "413         413      Bhoot    Horror   \n",
       "414         414      Bhoot    Horror   \n",
       "\n",
       "                                               reviews  Sentiment  \n",
       "0    Very good movie for youths and teenagers. That...          1  \n",
       "1    Very good direction and very good acting too. ...          1  \n",
       "2             not as sequel, good for individual watch          1  \n",
       "3                             very good movie to watch          1  \n",
       "4    I did not like Babadook, but this one is actua...          1  \n",
       "..                                                 ...        ...  \n",
       "410                        Best action of Vicky kausal          1  \n",
       "411  Movies has good storyline...it is an one time ...          1  \n",
       "412  Great horror story. Different from other horro...          1  \n",
       "413  Different from normal horror stories of bollyw...          1  \n",
       "414    may be considered as gud b coz of Gud star cast          1  \n",
       "\n",
       "[415 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Movie_name</th>\n      <th>Genre</th>\n      <th>reviews</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Peninsula</td>\n      <td>Thriller</td>\n      <td>Very good movie for youths and teenagers. That...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Peninsula</td>\n      <td>Thriller</td>\n      <td>Very good direction and very good acting too. ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Peninsula</td>\n      <td>Thriller</td>\n      <td>not as sequel, good for individual watch</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Peninsula</td>\n      <td>Thriller</td>\n      <td>very good movie to watch</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Come Play</td>\n      <td>Mystery</td>\n      <td>I did not like Babadook, but this one is actua...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>410</td>\n      <td>Bhoot</td>\n      <td>Horror</td>\n      <td>Best action of Vicky kausal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>411</th>\n      <td>411</td>\n      <td>Bhoot</td>\n      <td>Horror</td>\n      <td>Movies has good storyline...it is an one time ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>412</td>\n      <td>Bhoot</td>\n      <td>Horror</td>\n      <td>Great horror story. Different from other horro...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>413</td>\n      <td>Bhoot</td>\n      <td>Horror</td>\n      <td>Different from normal horror stories of bollyw...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>414</td>\n      <td>Bhoot</td>\n      <td>Horror</td>\n      <td>may be considered as gud b coz of Gud star cast</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>415 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movie_name = new_dataframe.Movie_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Peninsula', 'Come Play', 'The Secret: Dare To Dream',\n",
       "       'The War with grandpa', 'Parasite', '1917', 'The Outpost',\n",
       "       'Little Woman', 'Birds of  prey', 'Jojo Rabbit', 'The gentleman',\n",
       "       'A Beautiful Day in the neighbourhood', 'trolls world tour',\n",
       "       'The Witches', \"Never Steal A man's second chance\", 'Greenland',\n",
       "       'Antebellum', 'A secret : Dare to dream', 'The Silencing', 'Relic',\n",
       "       'BloodShot', 'Sonic the Hedgehog', 'Bad Boys for life',\n",
       "       'The Lasr Full measure', 'Just Mercy', 'Dolittle', 'Bombshell',\n",
       "       'The Lost city', 'Life is  Beautiful', 'Good will Hunting',\n",
       "       'The Shawshank redemption', 'Angry men', 'angry men',\n",
       "       'The Dark Knight', 'The Matrix', 'Forest Gump', 'America Beauty',\n",
       "       'The Godfather', 'the Departed', 'The Prestige', 'Gladiator',\n",
       "       'Inglourious Basterds', 'Million Dollar baby', 'The Aviator',\n",
       "       'Avatar', 'Big Fish', 'Meet Joe Black', 'The Town',\n",
       "       \"Schndler's List\", 'Titanic', 'spy game', 'psycho', 'pay check',\n",
       "       'Mission impossible II', 'Pay it forward', 'Invictus', 'Se7ev',\n",
       "       'Saving private Ryan', 'Cast Away', 'the last summer', 'Munich',\n",
       "       'The longest yard', 'Bruce Almighty', 'Inception', 'Sir',\n",
       "       'Chhalaang', 'Suraj pe Mangal Bhari', 'Ludo', 'Laxmii',\n",
       "       'Khaali peeli', 'Sadak 2', 'Khuda Haafiz',\n",
       "       'Gunjan Saxena: The Kargil Girl', 'Lootcase', 'Raat Akeli hai',\n",
       "       'Shakuntala Devi', 'Dil Bechara', 'Virgin Bhanupriya',\n",
       "       'Gulabo sitabo', 'Angrezi Medium', 'Baaghi 3', 'Rizwan', 'Thappad',\n",
       "       'Bhoot'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "Movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movie_name = Movie_name[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Peninsula', 'Come Play', 'The Secret: Dare To Dream',\n",
       "       'The War with grandpa', 'Parasite', '1917', 'The Outpost',\n",
       "       'Little Woman', 'Birds of  prey', 'Jojo Rabbit', 'The gentleman',\n",
       "       'A Beautiful Day in the neighbourhood', 'trolls world tour',\n",
       "       'The Witches', \"Never Steal A man's second chance\", 'Greenland',\n",
       "       'Antebellum', 'A secret : Dare to dream', 'The Silencing', 'Relic',\n",
       "       'BloodShot', 'Sonic the Hedgehog', 'Bad Boys for life',\n",
       "       'The Lasr Full measure', 'Just Mercy', 'Dolittle', 'Bombshell',\n",
       "       'The Lost city', 'Life is  Beautiful', 'Good will Hunting',\n",
       "       'The Shawshank redemption', 'Angry men', 'angry men',\n",
       "       'The Dark Knight', 'The Matrix', 'Forest Gump', 'America Beauty',\n",
       "       'The Godfather', 'the Departed', 'The Prestige', 'Gladiator',\n",
       "       'Inglourious Basterds', 'Million Dollar baby', 'The Aviator',\n",
       "       'Avatar', 'Big Fish', 'Meet Joe Black', 'The Town',\n",
       "       \"Schndler's List\", 'Titanic', 'spy game', 'psycho', 'pay check',\n",
       "       'Mission impossible II', 'Pay it forward', 'Invictus', 'Se7ev',\n",
       "       'Saving private Ryan', 'Cast Away', 'the last summer', 'Munich',\n",
       "       'The longest yard', 'Bruce Almighty', 'Inception', 'Sir',\n",
       "       'Chhalaang', 'Suraj pe Mangal Bhari', 'Ludo', 'Laxmii',\n",
       "       'Khaali peeli', 'Sadak 2', 'Khuda Haafiz',\n",
       "       'Gunjan Saxena: The Kargil Girl', 'Lootcase', 'Raat Akeli hai',\n",
       "       'Shakuntala Devi', 'Dil Bechara', 'Virgin Bhanupriya',\n",
       "       'Gulabo sitabo', 'Angrezi Medium', 'Baaghi 3', 'Rizwan', 'Thappad',\n",
       "       'Bhoot'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "Movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "415",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-37dd44266ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtemp_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mreview\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtemp_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 415"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "index = 0\n",
    "overall = []\n",
    "while count<=250:\n",
    "    temp_count = 0\n",
    "    reviews = 0\n",
    "    while temp_count < 3:\n",
    "        reviews += new_dataframe.Sentiment[index]\n",
    "        index += 1\n",
    "        temp_count += 1\n",
    "    if reviews/4 < 0.5:\n",
    "        overall.append(\"Negative\")\n",
    "    elif reviews/4 >= 0.5:\n",
    "        overall.append(\"positive\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}